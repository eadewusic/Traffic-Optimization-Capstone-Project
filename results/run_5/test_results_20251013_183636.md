# Test Results - 2025-10-13 18:36:39

## Model Information
- Model: `../models/hardware_ppo/run_5/final_model`
- VecNormalize: `../models/hardware_ppo/run_5/vecnormalize.pkl`

## Overall Performance

| Controller | Avg Reward | Avg Cleared | Final Queue |
|------------|------------|-------------|-------------|
| PPO (Retrained) | -781.3 | 244.0 | 25.0 |
| Longest Queue | -839.6 | 247.4 | 24.6 |
| Round Robin | -1253.5 | 229.0 | 30.0 |
| Fixed Time | -1618.6 | 218.8 | 39.6 |

## PPO vs Longest Queue Baseline
- Reward improvement: +6.9%
- Throughput improvement: -1.4%
- Queue reduction: -1.6%

## Wins by Controller
- PPO (Retrained): 3/5 scenarios
- Longest Queue: 2/5 scenarios
- Round Robin: 0/5 scenarios
- Fixed Time: 0/5 scenarios

## Champion: PPO (Retrained)

## Scenario Results

### Balanced Traffic
- PPO (Retrained): Reward=-521.7, Cleared=236, Queue=18
- Longest Queue: Reward=-769.9, Cleared=249, Queue=20
- Round Robin: Reward=-942.8, Cleared=234, Queue=22
- Fixed Time: Reward=-1738.3, Cleared=221, Queue=42

### North Heavy Congestion
- PPO (Retrained): Reward=-622.2, Cleared=239, Queue=26
- Longest Queue: Reward=-670.0, Cleared=240, Queue=23
- Round Robin: Reward=-956.4, Cleared=219, Queue=15
- Fixed Time: Reward=-1656.3, Cleared=232, Queue=46

### East-West Rush Hour
- PPO (Retrained): Reward=-1079.2, Cleared=246, Queue=40
- Longest Queue: Reward=-715.3, Cleared=248, Queue=11
- Round Robin: Reward=-1526.3, Cleared=220, Queue=27
- Fixed Time: Reward=-1688.8, Cleared=209, Queue=35

### Random Traffic Pattern
- PPO (Retrained): Reward=-783.8, Cleared=250, Queue=22
- Longest Queue: Reward=-1183.6, Cleared=250, Queue=39
- Round Robin: Reward=-1422.8, Cleared=245, Queue=55
- Fixed Time: Reward=-1467.4, Cleared=206, Queue=30

### Single Lane Blocked
- PPO (Retrained): Reward=-899.5, Cleared=249, Queue=19
- Longest Queue: Reward=-859.1, Cleared=250, Queue=30
- Round Robin: Reward=-1419.1, Cleared=227, Queue=31
- Fixed Time: Reward=-1542.3, Cleared=226, Queue=45

## Visualizations
- Comparison plot: `../visualizations\controller_comparison_20251013_183636.png`
- Scenario heatmap: `../visualizations\scenario_heatmap_20251013_183636.png`


## **Why Negative Rewards?**

The reward function uses penalties for congestion detection, resulting in a negative scale where less negative values indicate better performance. The PPO agent achieves -781.3 compared to the baseline's -839.6, representing a 6.9% improvement in congestion management.

However, the reward scale should show positive rewards for good performance. The current design is penalty-dominant, which obscures the agent's success. I'll retrain in run 6 with a rebalanced reward function where throughput rewards dominate congestion penalties, producing positive rewards for good performance while maintaining the same strategic priorities. This should take about 3 hours.

**Visualization:**
```
Reward Scale (Congestion Penalty System):
  -400 ████████████ Excellent performance
  -600 ████████     Good performance  
  -800 ██████       Acceptable (PPO: -781)
 -1000 ████         Poor (Baseline: -840)
 -1200 ██           Bad
 -1600              Terrible
```
